# Звезда

- Студент: Кулик Артур Игоревич, группа 3823Б1ПМоп3
- Технология: SEQ, MPI
- Вариант: 8

## 1. Введение
Для решения сложных вычислительных задач (hpc) на практике используются кластерные системы с большим количеством многоядерных процессоров. Для коммуникации между процессами, запущенными на кластере, используют технологию MPI (Message Passing Interface). Цель работы - реализовать виртуальную топологию "Звезда", где есть один корневой процесс, который связан со всеми периферийными процессами, которые не имеют прямой связи друг с другом, а обмениваются сообщениями только через центр звезды - корневой процесс.  

## 2. Постановка задачи
Задача: Реализовать виртуальную топологию "Звезда".

**Входные данные:** кортеж `std::tuple<int, int, std::vector<int>>`, где 0-й элемент - это номер процесса-отправителя, 1-й элемент - это номер процесса-получателя, 2-й элемент - это вектор, который содержит сами данные сообщения.  
**Выходные данные:** пересылаемое сообщение на процессе-получателе `std::vector<int>`.

**Ограничения к входным данным:** номера процессов от 0 до кол-во процессов (не включительно) + общее количество процессов не меньше 3-х.   

**Необходимо реализовать:**
1. последовательную (**SEQ**) и параллельную (**MPI**) версию алгоритма.
2. Покрытие кода тестами, проверяющими функционал и производительность. 

## 3. Описание последовательного алгоритма
В последовательном алгоритме копируем вектор из входных данных `GetInput()` и присваиваем выходным `GetOutput()`.

## 4. Схема распараллеливания
Для реализации MPI-программы используется топология стандартного коммуникатора `MPI_COMM_WORLD`. Нулевой процесс является центром звезды (корневым).

Разобьём алгоритм на несколько этапов:
1. Определение реального источника: поиск процесса, который фактически имеет данные для передачи. Проблема заключается в том, что только процесс-отправитель знает, что он отправитель, так как все входные данные лежат на нем. Делаем мы это с помощью функции `FindActualSourceRank`, проверяя на пустоту вектор данных сообщения.
2. С помощью функции `MPI_Bcast` рассылаем данные о процессе-отправителе, процессе-получателе и размере входных данных.
3. Обрабатываем случай, когда процесс-отправитель и процесс-получатель совпадают, в таком случае просто копируем входные данные в GetOutput(), с помощью функции `HandleSameSourceDestination(proc_rank, source_rank, size, std::get<2>(input), GetOutput())`.
4. Обрабатыаем случай, когда процесс-отправитель и процесс-получатель различны (основной случай), с помощью функции `HandleDifferentSourceDestination(proc_rank, source_rank, destination_rank, size, std::get<2>(input), GetOutput())`, которая в свою очередь обрабатывает ситуацию, когда один из процессов отправителя или получателя - корневой, вызывая функции: `ProcessZeroRouting`, `ProcessDestination`, `ProcessSource`.
## 5. Детали реализации
1. Основная информация о структуре решения (ввод/вывод) находится в файле `common.hpp`. `InType` - тип входных данных, в данном случае - `std::tuple<int, int, std::vector<int>>`, OutType - тип выходных данных, в нашем случае - `std::vector<int>`. `TestType` - тип входных данных для тестов, в данном случае - `std::string`, т.к в тестах считываются названия бинарных файлов. `BaseTask` - это базовый класс интерфейса.
2. В папке `data` находятся бинарные файлы: `0132.bin`, `1032.bin`, `1232.bin`, `2232.bin` и `perf.bin`. в файлах `0132.bin`, `1032.bin`, `1232.bin`, `2232.bin` 1-я цифра - процесс-отправитель, 2-я цифра - процесс-получатель, 32 - длина вектора данных. `perf.bin` имеет процесс-отправитель 1, процесс-получатель 2, размер вектора входных данных 1'000'000. Векторы были сгенерированы на локальной машине с помощью генератора псевдослучайных чисел `MT19937`, в качестве `seed` использовался `std::random_device`. Подставляем результат `MT19937` в `std::uniform_int_distribution<int> dist(-1000.0, 1000.0)`. Файлы содержат также размеры векторов. 
3. Были добавлены вспомогательные функции `FindActualSourceRank`, `HandleSameSourceDestination`, `ProcessZeroRouting`, `ProcessDestination`, `HandleDifferentSourceDestination` для упрощения логики основной функции `RunImpl`.

## 6. Конфигурация системы запуска
- локальная машина: intel core i5 9600k, 6 cores / 6 threads, 16GB DDR4, Windows 10
- Окружение и ключи: MSVC, 1931, Release, -O2, MS-MPI, mpiexec -n 4
- Данные: вектор длиной `1'000'000`, сгенерированный как описано в пункте 5.

## 7. Экспериментальные результаты

### 7.1 Корректность 
Корректность работы алгоритма проверена в тестах. 

### 7.2 Производительность

| Mode        | Count | Time, s | Speedup | Efficiency |
|-------------|-------|---------|---------|------------|
| seq         | 1     | 0.001   | 1.00    | N/A        |
| mpi         | 2     | 0.919   | 0.001   | 0.1%       |
| mpi         | 4     | 0.937   | 0.001   | 0.1%       |

По очевидным причинам последовательно работающий код (**SEQ**) намного быстрей проходит тесты, так как фактически он общается с самим собой (программа запускается на одном процессе). Количество процессов не должно влиять на скорость пересылки сообщения от одного процесса к другому / минимальное влияние на производительность.

## 8. Заключение
Была реализована виртуальная топология "Звезда". Также были проведены сравнения производительности для (**MPI**) на разном числе процессов, исходя из результатов, можем судить об успешном сохранении производительности программы при увеличении числа процессов (пересылка сообщений осуществляется с той же скоростью). Так же были предусмотрены всевозможные случаи входных данных и обработка их с помощью различных функциональных тестов. 

## 9. Источники
https://learn.microsoft.com/ru-ru/message-passing-interface/microsoft-mpi