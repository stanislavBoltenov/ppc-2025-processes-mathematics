# Топология решетка-тор

- Студент: Четверикова Екатерина Евгеньевна, группа 3823Б1ПМоп3
- Технология: SEQ, MPI
- Вариант: 9

## 1. Введение
В настоящее время в профильных областях широко используются кластерные системы, упрощенно представляющие собой набор вычислительных узлов с индивидуальной памятью. Написание программ под такие системы (ввиду их специфики) требует организации межузловой коммуникации и синхронизации вычислений.

Средства межпроцессового взаимодействия описывает стандарт MPI (*Message Passing Interface* - интерфейс передачи сообщений). В учебных целях с использованием библиотеки (программной реализации стандарта) *MPI* была решена задача организации обмена данными в соответствии с топологией "решетка-тор". Данная топология предполагает расположение вычислительных узлов в виде двумерной статической решётки, где каждый узел соединён с ближайшими соседями, а грани решётки «замкнуты» для образования тора. Так каждый узел имеет четырех соседей: двух по вертикали и двух по горизонтали. Такая структура обмена данными является фундаментальной для многих классов вычислительных задач, в частности, для решения уравнений в частных производных методом конечных разностей, моделирования физических процессов на регулярных сетках и реализации параллельных алгоритмов линейной алгебры.
Следует отметить, что задача носит ознакомительный характер, поскольку MPI предлагает встроенные средства разворачивания топологий коммуникатора, такие как MPI_Cart_Create.

## 2. Постановка задачи
Решается задача задача организации межпроцессового ваимодействия (обмена данными) в соответствии с топологией коммуникатора "решетка-тор".

**Входные данные:** кортеж `std::tuple<int, int, std::vector<double>>`, содержащий ранги отправителя и получателя, размер передаваемого сообщения и данные сообщения типа `double`.
**Выходные данные:** кортеж `std::tuple<std::vector<double>, std::vector<int>>` полученное сообщение `std::vector<double>` и путь `std::vector<int>>`, содержащий ранги процессов, по которым передавалось сообщение.

**Требования:**
1. Реализация *параллельной* (**MPI**) версии алгоритма без использования встроенных средств **MPI** по созданию топологий коммуникатора.
2. Покрытие кода *функциональными* и *performance*-тестами (**gtest**).
3. Соблюдение заданной структуры репозитория и тестового интерфейса.

## 3. Описание последовательного алгоритма 
Последовательная реализация представляет собой заглушку, возвращающую `true` на всех этапах. 

## 4. Схема распараллеливания
**Построение решётки:**
Для p процессов строится решётка rows × cols = p, где:
- rows — наибольший ближайший делитель p, не превышающий √p
- cols = p / rows

**Преобразование координат:**
row_ = rank / cols;
col_ = rank % cols;

row = ((row % rows_) + rows_) % rows_;
col = ((col % cols_) + cols_) % cols_;

rank = (row * cols_) + col;

**Алгоритм маршрутизации**
Данные передаются сначала по горизонтали (ось X), затем по вертикали (ось Y).

**Схема работы**
- каждый процесс вычисляет сетку
- процесс ранга 0 получает данные, передает остальным start и end
- каждый процесс независимо вычисляет полный маршрут
- данные передаются по цепочке (так как каждый знает только соседей)
- на процессе end на выходе форматируются непустые данные, согласно типу выходных данных
- остальные процессы возвращают пустые данные (сообщение и путь)
- синхронизация `MPI_Barrier`

## 5. Детали реализации

Архитектура проекта:
 - `common/include/common.hpp` содержит определение типов входных (`InType`) и выходных (`OutType`) данных, а также тип входных данных тестовых кейсов (`TestType`) и базовый класс интерфейса (`BaseTask`).

 - в директории `data` расположены текстовые файлы для функционального тестирования и тестирования производительности соответсвенно. Тестовые файлы сгенерированны отдельно использованием генератора псевдо-случайных чисел в отрезке от `-64.0` до `64.0`. Тестовые файлы проверяют как простую передачу данных (`test_base.txt`), так и обработку граничных случаев: передачу сообщений соседним узлам, переход через границу, передача самому себе. Каждый файл тестирования содержит данные определенной структуры: `<отправитель><получатель><сообщение>`.

 - В директориях `mpi` и `seq` расположены `.hpp` и `.cpp` файлы соответствующих реализаций. Общий интерфейс включает методы валидации данных `bool ValidationImpl(void)`, предобработки данных `bool PreProcessingImpl(void)`, выполнения алгоритма передачи сообщений в соответствии с топологией поставленной задачи `bool RunImpl(void)` и постобработки данных `bool PostProcessingImpl(void)` (не требуется).
    Для реализации топологии передачи сообщений "решетка-тор" и реализованы вспомогательные функции:
    - `DetermineGridDimensions` – подбирает наиболее сбалансированные (близкие к квадрату) размеры сетки rows и cols для заданного числа процессов.
    - `GetRank` – вычисляет ранг процесса по его координатам (row, col) в торической сетке.
    - `GetOptimalDirection` – определяет кратчайший путь между двумя узлами на решетки тора.
    - `ComputeNextNode` – находит следующий узел на пути от текущего к целевому, двигаясь сначала по столбцам, затем по строкам.
    - `ComputeFullPath` – строит полный маршрут (вектор рангов) от начального узла до конечного.
    - `ReceiveData` – принимает данные от указанного отправителя (сначала размер, затем вектор double).
    - `SendDataToNext` – отправляет данные следующему узлу в цепочке (сначала размер, затем данные).

 - Директория `tests` содержит функциональные тесты и тесты производительности с `main.cpp` реализациями в одноименных папках. Оба типа тестов имеют одинаковую структуру: в `void SetUp(void)` инициализации происходит чтение входных данных из файла, по завершении вычислений методом `bool CheckTestOutputData(OutType& output_data)` производится проверка корректности результата сравнением с входными данными.

 - Управление памятью осуществляется автоматически средствами контейнера `std::vector`. Динамические указатели с ручным управлением памятью не используются.

## 6. Тестовая конфигурация
- Процессор: 2th Gen Intel(R) Core(TM) i7-1255U   2.60 GHz  (2 cores, 12 threads)
- Память: 16,0 ГБ DDR4
- Операционная система: Windows 10 Pro 22H2
- Реализация MPI: MS-MPI Version 10.1.12498.16
- Параметры запуска: `mpiexec -n <count>`
- Данные: вектор размерности 8 млн элементов

## 7. Экспериментальные результаты

### 7.1 Корректность
Проверка корректности осуществлялась путем сравнения полученного сообщения с исходными данными отправителя, а так же совпадения начального и конечного узла пути с заданными узлами.

### 7.2 Производительность

|       Mode       | Count | Time, ms | Speedup | Efficiency |
|------------------|-------|----------|---------|------------|
| MPI_Cart_Create  | 4     | 150.74   | 1.00    | N/A        |
| mpi              | 4     | 151.85   | 0.99    | 99%        |

Полученные результаты показывают совсем небольшое **замедление** ручной реализации топологии "решетка-тор" MPI-коммуникатора. Выигрыш в производительности стандартных средств создания топологий объясняется лучшей оптимизацией коммуникаций и коллективных операций.


## 8. Заключение
В результате работы в учебных целях разработана параллельная (MPI) версии программы, выполняющей создание топологии коммуникатора типа "решетка-тор".
Проведенный в процессе выполнения работы эксперимент продемонстрировал незначительное снижение производительности рукописной версии. Эффективность представленной реализации с учетом округления составила 93% на 4 процессах.

## 9. Источники

MPI Standard: https://www.mpi-forum.org/docs/
OpenMPI Documentation: https://www.open-mpi.org/doc/