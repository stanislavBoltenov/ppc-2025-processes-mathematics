# Отчёт по задаче: Максимальное значение элементов матрицы

**Национальный исследовательский Нижегородский государственный университет им. Н.И. Лобачевского**  
Институт информационных технологий, математики и механики

| | |
|------|------|
| **Направление подготовки** | Прикладная математика и информатика |
| **Вариант задания** | №1 |
| **Студент** | Болтенков С.С. (группа 3823Б1ПМоп3) |
| **Преподаватель** | доцент Сысоев А.В. |
| **Год выполнения** | 2025 |


## Введение
Разработана функция broadcast - пересылка данных от одного MPI процесса всем остальынм. Проведён сравнительный анализ производительности с функцией MPI_Broadcast из библиотеки MPI.

## Постановка задачи
Нужно реализовать функцию broadcast, используя только функции Send и Recv. Реализованная функция должна иметь тот же прототип, что и MPI_Broadcast  из библиотеки MPI. Тестовая программа должна позволять выбрать номер процесса root и выполнять пересылку массива как минимум следующих типов: MPI_INT, MPI_FLOAT, MPI_DOUBLE. Во всех операциях передача должна выполняться с использованием «дерева» процессов.

Требуется:  
1. Разработать параллельную реализацию функции broadcast с использованием **MPI**.
2. Пересылку данных между процессами выполнять с использованием «дерева» процессов.  
3. Проверить корректность переслки данных.
4. Провести экспериментальные замеры времени выполнения.

## Описание алгоритма
1. Если процесс является корнем (root), то он отправляет данные своим потомкам (если они есть).
2. Если процесс не корень, то он сначала получает данные от родителя, а затем отправляет своим потомкам (если они есть).

## Описание MPI-версии
Программная реализация использует в качестве типа входных данных специализированный кортеж:  
`std::tuple<int, int, int, std::vector<char>>`, 
где:  
- первый элемент кортежа — номер процесса, который будет пересылать данные,  
- второй элемент — тип данных:
    - 0 — MPI_INT
    - 1 — MPI_FLOAT
    - 2 — MPI_DOUBLE
- третий элемент — количество элементов в массиве
- четвертый элемент - массив данных по байтам

Данная структура данных была выбрана как наиболее удобная для организации эффективного распределения данных между процессами и последующего сбора результатов вычислений.

## Результаты экспериментов и подтверждение корректности
Эксперименты проводились на локальной машине.  
Параметры тестовых данных:
- размер 10<sup>6</sup> элементов
- с элементами типов `int`, `float`, `double`.
- использовался генератор псевдослучайных чисел `mt19937`, также известный известный как **«Вихрь Мерсенна»**.


| Версия алгоритма | Время выполнения(с) 1 проц | Время выполнения(с) 2 проц | Время выполнения(с) 4 проц | Время выполнения(с) 8 проц |
|------------------:|---------------------:|---------------------:|---------------------:|---------------------:|
| из библиотеки MPI | 4.2e-06 | 0.0009545 | 0.0018566 | 0.0032967 |
| собственная реализация | 7.78e-05 | 0.0014911 | 0.0032726 | 0.0071103 |

**Подтверждение корректности:**  
Функция тестирования проверяет, что полученные процессом данные соответствуют отправленным данным.  
Все функциональные тесты и тесты производительности были успешно пройдены на локальной машине.

## Выводы из результатов
Собственная реализация работает приблизительно в 2.5 раза медленнее реализации из библиотеки MPI.

## Заключение
В работе реализована и протестирована функция broadcast, которая выполняет пересылку данных от одного процесса всем остальным. Проведённые эксперименты подтвердили корректность реализации функции и показали, что собственная реализация работает медленнее реализации из библиотеки MPI.

## Список литературы
1. Документация в формате веб-сайта по реализации **MPICH** стандарта **MPI**: [https://www.mpich.org](https://www.mpich.org)


## Параллельная реализация
```cpp
bool BoltenkovSBroadcastkMPI::CheckLeftChild(int left_child, int shift_left_child, MPI_Comm comm) {
  int rank = 0;
  int size = 0;
  MPI_Comm_rank(comm, &rank);
  MPI_Comm_size(comm, &size);
  return (left_child < size || shift_left_child < size) && left_child != rank;
}

bool BoltenkovSBroadcastkMPI::CheckRightChild(int right_child, int shift_right_child, MPI_Comm comm) {
  int rank = 0;
  int size = 0;
  MPI_Comm_rank(comm, &rank);
  MPI_Comm_size(comm, &size);
  return (right_child < size || shift_right_child < size) && right_child != rank;
}

int BoltenkovSBroadcastkMPI::MyBcast(void *buffer, int count, MPI_Datatype datatype, int root, MPI_Comm comm) {
  int rank = 0;
  int size = 0;
  MPI_Comm_rank(comm, &rank);
  MPI_Comm_size(comm, &size);

  if (size == 1) {
    return MPI_SUCCESS;
  }

  // for root = 0
  int shift_rank = (rank - root + size) % size;
  int shift_parent = (shift_rank == 0) ? -1 : (shift_rank - 1) / 2;
  int shift_left_child = (2 * shift_rank) + 1;
  int shift_right_child = (2 * shift_rank) + 2;

  // for cur root
  int parent = (shift_parent + root) % size;
  int left_child = (shift_left_child + root) % size;
  int right_child = (shift_right_child + root) % size;

  if (rank != root && shift_parent >= 0 && parent < size) {
    MPI_Recv(buffer, count, datatype, parent, 0, comm, MPI_STATUS_IGNORE);
  }

  if (CheckLeftChild(left_child, shift_left_child, comm)) {
    MPI_Send(buffer, count, datatype, left_child, 0, comm);
  }

  if (CheckRightChild(right_child, shift_right_child, comm)) {
    MPI_Send(buffer, count, datatype, right_child, 0, comm);
  }

  return MPI_SUCCESS;
}
```