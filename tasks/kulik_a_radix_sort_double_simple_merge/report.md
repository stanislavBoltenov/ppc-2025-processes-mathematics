# Поразрядная сортировка для вещественных чисел (тип double) с простым слиянием

- Студент: Кулик Артур Игоревич, группа 3823Б1ПМоп3
- Технология: SEQ, MPI
- Вариант: 20

## 1. Введение
Для решения сложных вычислительных задач (hpc) на практике используются кластерные системы с большим количеством многоядерных процессоров. Для коммуникации между процессами, запущенными на кластере, используют технологию MPI (Message Passing Interface). Цель работы - реализовать параллельный алгоритм поразрядной сортировки вещественных чисел типа `double` с использованием MPI для уменьшения времени сортировки больших векторов на нескольких процессах. 

## 2. Постановка задачи
Задача: Реализовать поразрядную сортировку по возрастанию для вещественных чисел (тип double) с простым слиянием. 

**Входные данные:** вектор `std::vector<double>`.
**Выходные данные:** отсортированный вектор `std::vector<double>`.

**Ограничения к входным данным:** входной вектор не пустой.

**Необходимо реализовать:**
1. последовательную (**SEQ**) и параллельную (**MPI**) версию алгоритма.
2. Покрытие кода тестами, проверяющими функционал и производительность. 

## 3. Описание последовательного алгоритма
Формализируя задачу, необходимо сделать такую перестановку элементов вектора, что: 
v[i - 1] <= v[ i ] для i = 1, ..., (n-1); где n - это размер вектора.
Поразрядная сортировка (`LSD` - `Least Significant Digit`) для чисел `double` выполняется по-байтно (размер `double` составляет `8 байт`), начиная с младшего байта. Так же необходимо учитывать знак вещественного числа (хранится в старшем бите): после сортировки по байтам отрицательные числа оказываются в обратном порядке, поэтому требуется корректировка их порядка.
Разобьём алгоритм на несколько этапов:
1. Выделяется буфер такого же размера, как исходный вектор.
2. Для каждого байта в числе типа `double`: считаем, сколько раз встречается конкретное значение байта [0, 2^8 - 1 == 255], записываем в `std::vector<uint64_t> count`, затем преобразуем подсчёты в позиции для записи (обновляется `std::vector<uint64_t> count`), после этого перераспределяем элементы массива в буфер `pbuffer`. В завершении, меняем указатели на `parr` и `pbuffer` местами, таким образом `parr` указывает на массив уже отсортированный по текущему байту, а `pbuffer` будет использовать как будет в следующей итерации.
3. После обработки всех байтов перемещаем отрицательные числа в начало массива, сохраняя при этом их относительный порядок.

## 4. Схема распараллеливания
Для реализации MPI-программы используется топология стандартного коммуникатора `MPI_COMM_WORLD`. Нулевой процесс является корневым.

Разобьём алгоритм на несколько этапов:
1. Распределение данных: нулевой процесс рассылает размер глобального массива всем процессам с помощью функции `MPI_Bcast`, так же обрабатывается случай, когда размер вектор не кратен количеству процессов `proc_num`: первые `r = global_size % proc_num` процессов получают на один элемент больше. Далее функцией `MPI_Scatterv` отправляем соответствующие части разбиения каждому процессу.
2. Каждый процесс выполняет локальную поразрядную сортировку `LSDSortLocal` для своих данных.
3. Собираем все отсортированные части исходного вектора на корневом процессе с функции `MPI_Gather`.
4. Функция обработки граничных случаев `CheckBoundaries` каждый процесс (кроме последнего) отправляет свой последний элемент следующему процессу, каждый процесс (кроме нулевого) принимает последний элемент от предыдущего процесса. Порядок отправки и принятия изменён (сначала `MPI_Recv`, затем `MPI_Send`) с целью избежать дедлока. Проверяется разность между граничными элементами соседних процессов, при необходимости обновляется локальный максимум и индекс.
4. Нулевой процесс выполняет простое слияние локально отсортированных массивов функцией `SimpleMerge`.

## 5. Детали реализации
1. Основная информация о структуре решения (ввод/вывод) находится в файле `common.hpp`. `InType` - тип входных данных, в данном случае - `std::vector<double>`, OutType - тип выходных данных, в нашем случае - `std::vector<double>`. `TestType` - тип входных данных для тестов, в данном случае - `std::string`, т.к в тестах считываются названия бинарных файлов. `BaseTask` - это базовый класс интерфейса.
2. В папке `data` находятся бинарные файлы: `vector1.bin`, `vector2.bin` и `vector3.bin`. `vector1` имеет размер `100'000` и содержит числа формата `double`, `vector2` имеет размер `2'000'005` и содержит числа формата `double`, `vector3` имеет размер `3` и содержит числа формата `double` - нужен для проверки случая, когда `n` < `proc_num`. Векторы были сгенерированы на локальной машине с помощью генератора псевдослучайных чисел `MT19937`, в качестве `seed` использовался `std::random_device`. Подставляем результат `MT19937` в `std::uniform_real_distribution<double> dist(-1000.0, 1000.0)` - равномерное вещественное распределение на отрезке `[-1000; 1000]`. Файлы содержат также размеры векторов. 
3. Были добавлены вспомогательные функции `LSDSortBytes`, `AdjustNegativeNumbers`, `LSDSortLocal`, `SimpleMerge`, `LSDSortDouble` для упрощения логики основной функции `RunImpl`.

## 6. Конфигурация системы запуска
- локальная машина: intel core i5 9600k, 6 cores / 6 threads, 16GB DDR4, Windows 10
- Окружение и ключи: MSVC, 1931, Release, -O2, MS-MPI, mpiexec -n 4
- Данные: вектор длиной `2'000'005`, сгенерированный как описано в пункте 5.

## 7. Экспериментальные результаты

### 7.1 Корректность 
Корректность работы алгоритма проверена в тестах. 

### 7.2 Производительность

| Mode        | Count | Time, s | Speedup | Efficiency |
|-------------|-------|---------|---------|------------|
| seq         | 1     | 0.471   | 1.00    | N/A        |
| mpi         | 2     | 0.328   | 1.44    | 72.0%      |
| mpi         | 4     | 0.216   | 2.18    | 54.5%      |

Мы можем видеть хороший прирост производительности при увеличении количества процессов. Это говорит нам о том, что алгоритм хорошо параллелится и временные затраты на обслуживание MPI окупаются параллельной работой программы.

## 8. Заключение
Были реализованы последовательная (**SEQ**) и параллельная (**MPI**) версия алгоритма. Также были проведены сравнения производительности данных реализаций, исходя из результатов, можем судить об успешном распараллеливании программы. Запуск на 4-х процессах ещё увеличил ускорение, получаемое на 2-х процессах. Из этого можем высказать предположение, что ресурс параллелизма ещё не исчерпан, вполне возможно получить большее ускорение на большем числе процессов. 

## 9. Источники
https://learn.microsoft.com/ru-ru/message-passing-interface/microsoft-mpi
https://hpc-education.unn.ru/files/courses/XeonPhi/Lab07.pdf?ysclid=mjiu3zsvm5697841136