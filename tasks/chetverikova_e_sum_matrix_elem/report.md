# Сумма элементов матрицы

- Студент: Четверикова Екатерина Евгеньевна, группа 3823Б1ПМоп3
- Технология: SEQ, MPI
- Вариант: 10


## 1. Введение

Обработка больших матриц является частой задачей в научных вычислениях. Параллельное программирование с использованием MPI позволяет распределить вычисления между несколькими процессами, значительно ускоряя обработку. В работе реализуется и сравнивается эффективность последовательного и параллельного подхода к суммированию элементов матрицы. 
Задача лабораторной работы — ознакомиться с принципами работы MPI, поэтому значительного ускорения от реализации алгоритма суммирования не ожидается.


## 2. Постановка задачи

Решается задача нахождения суммы элементов матрицы заданной размерности, в вещественных координатах.

**Входные данные:** кортеж, описывающий матрицу — количество строк, столбцов и вектор, в котором хранится матрица по строкам std::tuple<uint32_t, uint32_t, std::vector<double>>.
**Выходные данные:** результат суммы типа `double`.

**Ограничения:**
1. Количество строк и столбцов матрицы больше нуля.
2. Размер вектора, в котором хранится матрица равен произведению количества столбцов и строк.
2. Матрица заполнена элементами (не пустая).


## 3. Описание последовательного алгоритма 

Алгоритм последовательного суммирования реализован с использованием цикла range-based for, который последовательно перебирает все элементы value вектора matrix. Результат суммируется в переменную res. Процесс продолжается до полного перебора элементов вектора.

for (double value : matrix) {
    res += value;
} 


## 4. Схема распараллеливания

Параллельный алгоритм реализует распределение блоков данных между MPI-процессами. Реализация выполнена с использованием стандартного коммуникатора `MPI_COMM_WORLD`. В качестве корневого процесса используется процесс с рангом 0.

Алгоритм состоит из нескольких этапов:
1.  Данные распределяются равномерно по процессам посредством вызова функции `MPI_Scatter`. Размер данных определяется как `размер входных данных` / `количество процессов`, где / — целочисленное деление.
2. Каждый процесс вычисляет частичную сумму и сохраняет ее в локальной переменной.
3. Выполняется сбор данных с каждого процесса путем использования функции `MPI_Reduce` с параметром редуцирования `MPI_SUM`.
4. Обрабатывается хвост (при его наличии) процессом с рангом 0.
5. Результат суммы рассылается всем процессам с помощью функции `MPI_Bcast` в целях синхронизации данных для прохождения тестов (тесты а требуют наличие результата на всех процессах).


## 5. Детали реализации

Архитектура проекта:

 - `common/include/common.hpp` содержит определение типов входных (`InType`) и выходных (`OutType`) данных, а также тип входных данных тестовых кейсов (`TestType`; в данном случае строка `std::string`, представляющая имя файла с тестовыми данными) и базовый класс интерфейса (`BaseTask`).

 - в директории `data` расположены текстовые и бинарные файлы для функционального тестирования и тестирования производительности соответсвенно. Тестовые файлы `test2.txt`, `perf_2000.bin` сгенерированны отдельно использованием генератора псевдо-случайных чисел в отрезке от 0 до 1. За эталонный принят результат, подсчитанный методом `np.sum` из библиотеки `numpy`. Тестовый файл `test1.txt` создан самостоятельно.

 - В директориях `mpi` и `seq` расположены `.hpp` и `.cpp` файлы соответствующих реализаций. Общий интерфейс включает методы валидации данных `bool ValidationImpl(void)`, предобработки данных `bool PreProcessingImpl(void)`, вычисления  суммы элементов `bool RunImpl(void)` и постобработки данных `bool PostProcessingImpl(void)`.

 - Директория `tests` содержит функциональные тесты и тесты производительности с `main.cpp` реализациями в одноименных папках. Оба типа тестов имеют одинаковую структуру: в `void SetUp(void)` инициализации происходит чтение входных данных из файла, по завершении вычислений методом `bool CheckTestOutputData(OutType& output_data)` производится проверка корректности результата сравнением с эталонным значением (с поправкой на вычислительную погрешность машинной арифметики, оценки погрешности получены экспериментально).


## 6. Тестовая конфигурация

- Процессор: 2th Gen Intel(R) Core(TM) i7-1255U   2.60 GHz  (2 cores, 12 threads)
- Память: 16,0 ГБ DDR4
- Операционная система: Windows 10 Pro 22H2
- Реализация MPI: MS-MPI Version 10.1.12498.16
- Параметры запуска: `mpiexec -n <count>`
- Данные: матрица размерности 5000x5000


## 7. Экспериментальные результаты

Эксперименты проводились на локальной машине. Для оценки эффективности параллельного алгоритма тест проводился матрице 5000×5000 (в директории загружена другая матрица размерности 2000x2000 из-за ограничений на размер коммита).

| Mode        | Count | Time, s  | Speedup | Efficiency |
|-------------|-------|----------|---------|------------|
| seq         | 1     | 0.0516   | 1.00    | N/A        |
| mpi         | 2     | 0.0607   | 0.85    | 42.5%      |
| mpi         | 4     | 0.0745   | 0.69    | 17.3%      |
| mpi         | 6     | 0.0854   | 0.60    | 10.7%      |

Полученные результаты показывают *замедление* MPI-реализации алгоритма суммирования матрицы на машине с общей памятью. 
Наблюдается явление антимасштабируемости, характерное для задач с низким отношением вычислений к объему коммуникаций. Накладные расходы MPI превосходят вычислительную нагрузку, что замедляет программу. Наилучшая производительность достигнута при использовании 2 процессов, что соответствует числу ядер.


## 8. Заключение

В результате работы в учебных целях разработаны последовательная (SEQ) и параллельная (MPI) версии программы, вычисляющей сумму элементов матрицы.

Тестирование на данных больших размеров показало неэффективность использования параллельных алгоритмов в алгоритме суммирования элементов матрицы. Эффективность не превосходит 42.5%, лучшая производительность достигается на 2 процессах.


## 9. Источники

MPI Standard: https://www.mpi-forum.org/docs/
OpenMPI Documentation: https://www.open-mpi.org/doc/