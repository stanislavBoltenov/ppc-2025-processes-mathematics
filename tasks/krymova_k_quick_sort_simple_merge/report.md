# Отчет по реализации быстрой сортировки с простым слиянием

**Студент**: Крымова Кристина Дмитриевна, 3823Б1ПМоп3  
**Технология**: SEQ, MPI  
**Вариант**: 14 

## 1. Введение
- **Мотивация**: Изучить применение MPI для параллельной реализации быстрой сортировки с простым слиянием, оценить эффективность распределения вычислений между процессами.
- **Проблема**: Задача сортировки больших массивов данных является фундаментальной в компьютерных науках. При работе с большими объемами данных последовательная быстрая сортировка может быть недостаточно быстрой.
- **Ожидаемый результат**: MPI версия должна показать ускорение за счет распределения данных между процессами, их параллельной сортировки и последующего слияния.

## 2. Постановка задачи
На вход программе подается вектор целых чисел произвольного размера. Требуется отсортировать его в порядке возрастания, используя алгоритм быстрой сортировки с простым слиянием.

## 3. Базовый алгоритм (последовательный)
Последовательный алгоритм использует итеративную быструю сортировку с выбором медианы из трех элементов.

```cpp
inline void QuickSortIterative(std::vector<int> &arr) {
  if (arr.size() <= 1) {
    return;
  }

  struct StackItem {
    int left;
    int right;
  };

  std::vector<StackItem> stack{{0, static_cast<int>(arr.size()) - 1}};

  while (!stack.empty()) {
    const auto [left, right] = stack.back();
    stack.pop_back();

    if (left >= right) {
      continue;
    }

    const int mid = left + ((right - left) / 2);

    SortThreeElements(arr, left, mid, right);
    std::swap(arr[mid], arr[right]);
    const int pivot = arr[right];

    int i = left - 1;
    for (int j = left; j < right; ++j) {
      if (arr[j] <= pivot) {
        ++i;
        std::swap(arr[i], arr[j]);
      }
    }
    std::swap(arr[i + 1], arr[right]);
    const int partition = i + 1;

    if (partition - left > right - partition) {
      stack.push_back({left, partition - 1});
      stack.push_back({partition + 1, right});
    } else {
      stack.push_back({partition + 1, right});
      stack.push_back({left, partition - 1});
    }
  }
}

## 4. Описание параллельного алгоритма

Параллельный алгоритм реализует подход с использованием MPI и включает следующие этапы:

### 4.1 Распределение данных
- **Определение размера**: Корневой процесс (ранг 0) определяет общий размер массива
- **Распределение нагрузки**: Массив разбивается на приблизительно равные части с учетом остатка
- **Передача данных**: Каждый процесс получает свою часть через `MPI_Scatterv`

### 4.2 Локальная сортировка
- **Независимая обработка**: Каждый процесс независимо сортирует свою часть итеративной быстрой сортировкой
- **Оптимизация сортировки**: Используется алгоритм с выбором медианы из трех элементов для улучшения производительности

### 4.3 Сбор результатов
- **Концентрация данных**: Все процессы отправляют отсортированные части корневому процессу через `MPI_Gatherv`
- **Накопление результатов**: Корневой процесс собирает все отсортированные части в единый массив

### 4.4 Слияние результатов
- **Последовательное слияние**: Корневой процесс последовательно сливает полученные части с использованием `std::inplace_merge`

### 4.5 Распространение результата
- **Широковещательная рассылка**: Корневой процесс рассылает итоговый отсортированный массив всем процессам через `MPI_Bcast`
- **Синхронизация состояния**: Все процессы получают полный отсортированный массив для обеспечения согласованности

## 5. Experimental Setup

### 5.1 Конфигурация оборудования
- **Аппаратная платформа**: Apple MacBook Air с чипом Apple M1
- **Процессор**: 8 ядер (4 производительных + 4 энергоэффективных)
- **Оперативная память**: 8 GB RAM
- **Операционная система**: macOS

### 5.2 Инструментарий разработки
- **Компилятор**: Apple Clang version 16.0.0
- **Уровень оптимизации**: Сборка с флагом -O2
- **Библиотека MPI**: OpenMPI

### 5.3 Тестовые данные
- **Размер массива**: 100,000 элементов
- **Тип данных**: целые числа (int)
- **Распределение данных**: комбинированное (первая половина - убывающая последовательность, вторая половина - псевдослучайные числа)

## 6. Результаты

### 6.1 Проверка корректности
Корректность реализации обоих алгоритмов подтверждена 22 функциональными тестами, включающими:
- Пустые массивы и массивы из одного элемента
- Упорядоченные по возрастанию и убыванию последовательности
- Массивы с одинаковыми элементами
- Случайные данные различного размера (от 2 до 5000 элементов)

### 6.2 Анализ производительности
Базовое время выполнения последовательной (SEQ) версии составило **0.013621 секунд**.

#### Таблица результатов параллельного выполнения:

| Количество процессов | Время MPI (сек) | Ускорение | Данных на процесс |
|---------------------|-----------------|-----------|-------------------|
| 1                   | 0.013698        | 0.99x     | 100,000           |
| 2                   | 0.013465        | 1.01x     | 50,000            |
| 4                   | 0.004998        | 2.72x     | 25,000            |
| 6                   | 0.022307        | 0.61x     | 16,666            |
| 8                   | 0.008727        | 1.56x     | 12,500            |

### 6.3 Интерпретация результатов

#### Положительные аспекты реализации:
1. **Минимальные накладные расходы MPI**: 
   - При использовании 1 процесса наблюдается разница всего в 0.57% по сравнению с SEQ версией
   - При 2 процессах производительность практически идентична последовательной версии

2. **Эффективное использование ресурсов**:
   - Максимальное ускорение 2.72x достигается при 4 процессах
   - Ускорение 1.56x при 8 процессах демонстрирует потенциал для масштабирования

3. **Простота и надежность**: Алгоритм демонстрирует стабильную работу при различном количестве процессов

#### Обнаруженные ограничения:
1. **Проблема с несбалансированной нагрузкой**:
   - При 6 процессах наблюдается значительное замедление (61% от производительности SEQ версии)
   - Основная причина: неравномерное распределение данных (100,000 / 6 = 16,666 с остатком 4)

2. **Ограничения масштабирования**:
   - Размер данных (100,000 элементов) может быть недостаточным для эффективного использования 6-8 процессов
   - Увеличение коммуникационных затрат при росте числа процессов

#### Рекомендации по оптимизации:
1. **Динамическая балансировка нагрузки**: Использование алгоритмов динамического распределения данных
2. **Адаптивный выбор количества процессов**: Автоматический подбор оптимального количества процессов в зависимости от размера данных
3. **Улучшение алгоритма слияния**: Использование параллельных алгоритмов слияния для распределения нагрузки

## 7. Выводы

1. **Достигнутое ускорение**: Параллельная реализация демонстрирует ускорение (до 2.72x) при оптимальной конфигурации

2. **Важность балансировки нагрузки**: Результаты подчеркивают критическую важность равномерного распределения данных между процессами

3. **Практическая применимость**: MPI-реализация эффективна для сортировки средних и больших массивов данных при правильном выборе количества процессов

4. **Перспективы улучшения**: Возможности для дальнейшей оптимизации включают улучшение алгоритмов распределения данных и слияния результатов

## 8. Литература
1. Лекционные материалы по параллельному программированию

## 9. Приложение
```cpp
bool KrymovaKQuickSortSimpleMergeMPI::RunImpl() {
  int rank = 0;
  int size = 0;
  MPI_Comm_rank(MPI_COMM_WORLD, &rank);
  MPI_Comm_size(MPI_COMM_WORLD, &size);

  int total_size = 0;
  if (rank == 0) {
    total_size = static_cast<int>(GetInput().size());
  }

  MPI_Bcast(&total_size, 1, MPI_INT, 0, MPI_COMM_WORLD);

  if (total_size == 0) {
    GetOutput() = std::vector<int>();
    return true;
  }

  int base_chunk = total_size / size;
  int remainder = total_size % size;

  std::vector<int> send_counts(size, base_chunk);
  std::vector<int> send_displs(size, 0);

  for (int i = 0; i < remainder; ++i) {
    send_counts[i]++;
  }

  for (int i = 1; i < size; ++i) {
    send_displs[i] = send_displs[i - 1] + send_counts[i - 1];
  }

  int local_size = 0;
  local_size = send_counts[rank];
  std::vector<int> local_data(local_size);

  if (rank == 0) {
    MPI_Scatterv(GetInput().data(), send_counts.data(), send_displs.data(), MPI_INT, local_data.data(), local_size,
                 MPI_INT, 0, MPI_COMM_WORLD);
  } else {
    MPI_Scatterv(nullptr, nullptr, nullptr, MPI_INT, local_data.data(), local_size, MPI_INT, 0, MPI_COMM_WORLD);
  }

  if (!local_data.empty()) {
    QuickSortIterative(local_data);
  }

  std::vector<int> result;

  if (rank == 0) {
    result.resize(total_size);
  }

  MPI_Gatherv(local_data.data(), local_size, MPI_INT, (rank == 0) ? result.data() : nullptr, send_counts.data(),
              send_displs.data(), MPI_INT, 0, MPI_COMM_WORLD);

  if (rank == 0) {
    for (int i = 1; i < size; ++i) {
      int start = 0;
      start = send_displs[i];
      int end = 0;
      end = start + send_counts[i];
      std::inplace_merge(result.begin(), result.begin() + start, result.begin() + end);
    }
  }

  int result_size = 0;
  if (rank == 0) {
    result_size = static_cast<int>(result.size());
  }

  MPI_Bcast(&result_size, 1, MPI_INT, 0, MPI_COMM_WORLD);

  if (rank != 0) {
    result.resize(result_size);
  }

  if (result_size > 0) {
    MPI_Bcast(result.data(), result_size, MPI_INT, 0, MPI_COMM_WORLD);
  }
  GetOutput() = result;

  return true;
}
