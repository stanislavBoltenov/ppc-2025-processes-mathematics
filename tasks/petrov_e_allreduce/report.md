#  	Передача от всех одному и рассылка (allreduce)

- Студент: Петров Егор Владимирович, группа 3823Б1ПМоп3
- Технология: MPI
- Вариант: 3

## 1. Введение
Работа с большими матрицами часто требует много времени. Последовательные алгоритмы испытывают проблемы с производительностью и не превосходят ожидания пользователя. Это происходит из-за того, что программа использует не всю доступную ей вычислительную мощность. Параллельные версии должны ускорить работу программы, задействовав все доступные ресурсы аппаратного обеспечения.

## 2. Постановка задачи
Требуется реализовать алгоритм MPI_Allreduce. Обязательное условие реализации - использование только MPI_Send и MPI_Recv. Работа алгоритма рассматривается на предыдущей задаче - поиска максимума по столбцам матрицы.

## 3. Базовый Алгоритм
При реализации алгоритма Allreduce пользуемся деревом процессов:
1. Инициализируем ранги и кол-во процессов
2. Строим дерево процессов
3. Если есть левый потомок:
    3.1. Принимаем буфер
    3.2. Проводим над ним операцию
4. Если есть правый потомок:
    4.1. Принимаем буфер
    4.2. Проводим над ним операцию
5. Если не корневой процесс:
    5.1. Отправляем буфер
    5.2. Принимаем готовый буфер
6. Если есть левый потомок:
    6.1. Отправляем готовый буфер
7. Если есть правый потомок:
    7.1. Отправляем готовый буфер

## 4. Схема распараллеливания
В реализации используется дерево процессов. Сначала идет сбор данных снизу вверх, параллельно выполняя операцию над ними, затем идет рассылка всем процессам сверху вниз.

## 5. Детали реализации
Чтобы отличать сообщения с разными буферами используются теги.

## 6. Характеристики локальной машины
- Аппаратное обеспечение/Операционная система: Intel Core i5 14400F, 10 ядер/16 потоков, 32 GB DDR5-5600, Windows 11 25H2.
- Компилятор: MSVC, Тип сборки: Release.
- Данные: сгенерированная матрица размерами 3000 на 3000 с элементами от -1e-4 до 1e-4.

## 7. Результаты

### 7.1 Корректность
Корректность программы была проверена с помощью функциональных тестов на разных типах матриц с разными данными.

### 7.2 Производительность
Для теста производительности была использована сгенерированная матрица размерами 3000 на 3000 с элементами от -1e-4 до 1e-4. Результаты были записаны в таблицу:

| Mode        | Count | Time, s | Speedup | Efficiency |
|-------------|-------|---------|---------|------------|
| seq         | 1     | 0.0197  | 1.00    | N/A        |
| mpi         | 2     | 0.0120  | 1.64    | 82.0%      |
| mpi         | 3     | 0.0115  | 1.04    | 34.7%      |
| mpi         | 4     | 0.0109  | 1.05    | 26.3%      |

## 8. Выводы
Была реализован собственный алгоритм MPI_Allreduce, который уступает готовой реализации.

## 9. Список литературы
1. https://learn.microsoft.com/ru-ru/message-passing-interface/microsoft-mpi
2. https://learn.microsoft.com/ru-ru/message-passing-interface/mpi-allreduce-function
3. https://learn.microsoft.com/ru-ru/message-passing-interface/mpi-send-function
4. https://learn.microsoft.com/ru-ru/message-passing-interface/mpi-recv-function